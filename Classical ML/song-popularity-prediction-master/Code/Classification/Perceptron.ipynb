{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.neighbors._base\n",
    "from sklearn.utils import _safe_indexing\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, r2_score, mean_squared_error\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the modified dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/haochenyang/Desktop/EECS545/Project/data_merged_quartile.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>popularity</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>...</th>\n",
       "      <th>genre_2</th>\n",
       "      <th>genre_3</th>\n",
       "      <th>genre_4</th>\n",
       "      <th>genre_5</th>\n",
       "      <th>genre_6</th>\n",
       "      <th>genre_7</th>\n",
       "      <th>genre_8</th>\n",
       "      <th>genre_9</th>\n",
       "      <th>Minor</th>\n",
       "      <th>Major</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01270</td>\n",
       "      <td>0.622</td>\n",
       "      <td>218293.0</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.124</td>\n",
       "      <td>-7.043</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>115.002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00306</td>\n",
       "      <td>0.620</td>\n",
       "      <td>215613.0</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.011800</td>\n",
       "      <td>0.534</td>\n",
       "      <td>-4.617</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>127.994</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02540</td>\n",
       "      <td>0.774</td>\n",
       "      <td>166875.0</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.002530</td>\n",
       "      <td>0.157</td>\n",
       "      <td>-4.498</td>\n",
       "      <td>0.2390</td>\n",
       "      <td>128.014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00465</td>\n",
       "      <td>0.638</td>\n",
       "      <td>222369.0</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.909000</td>\n",
       "      <td>0.157</td>\n",
       "      <td>-6.266</td>\n",
       "      <td>0.0413</td>\n",
       "      <td>145.036</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.02890</td>\n",
       "      <td>0.572</td>\n",
       "      <td>214408.0</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-4.294</td>\n",
       "      <td>0.3510</td>\n",
       "      <td>149.995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   popularity  acousticness  danceability  duration_ms  energy  \\\n",
       "0         0.0       0.01270         0.622     218293.0   0.890   \n",
       "1         0.0       0.00306         0.620     215613.0   0.755   \n",
       "2         0.0       0.02540         0.774     166875.0   0.700   \n",
       "3         0.0       0.00465         0.638     222369.0   0.587   \n",
       "4         2.0       0.02890         0.572     214408.0   0.803   \n",
       "\n",
       "   instrumentalness  liveness  loudness  speechiness    tempo  ...  genre_2  \\\n",
       "0          0.950000     0.124    -7.043       0.0300  115.002  ...      0.0   \n",
       "1          0.011800     0.534    -4.617       0.0345  127.994  ...      0.0   \n",
       "2          0.002530     0.157    -4.498       0.2390  128.014  ...      0.0   \n",
       "3          0.909000     0.157    -6.266       0.0413  145.036  ...      0.0   \n",
       "4          0.000008     0.106    -4.294       0.3510  149.995  ...      0.0   \n",
       "\n",
       "   genre_3  genre_4  genre_5  genre_6  genre_7  genre_8  genre_9  Minor  Major  \n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0    0.0    1.0  \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0    1.0    0.0  \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0    1.0    0.0  \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0    1.0    0.0  \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0    1.0    0.0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing the data and its basic statistical properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>popularity</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>...</th>\n",
       "      <th>genre_2</th>\n",
       "      <th>genre_3</th>\n",
       "      <th>genre_4</th>\n",
       "      <th>genre_5</th>\n",
       "      <th>genre_6</th>\n",
       "      <th>genre_7</th>\n",
       "      <th>genre_8</th>\n",
       "      <th>genre_9</th>\n",
       "      <th>Minor</th>\n",
       "      <th>Major</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40556.000000</td>\n",
       "      <td>40556.000000</td>\n",
       "      <td>40556.000000</td>\n",
       "      <td>4.055600e+04</td>\n",
       "      <td>40556.000000</td>\n",
       "      <td>40556.000000</td>\n",
       "      <td>40556.000000</td>\n",
       "      <td>40556.000000</td>\n",
       "      <td>40556.000000</td>\n",
       "      <td>40556.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>40556.000000</td>\n",
       "      <td>40556.000000</td>\n",
       "      <td>40556.000000</td>\n",
       "      <td>40556.000000</td>\n",
       "      <td>40556.000000</td>\n",
       "      <td>40556.000000</td>\n",
       "      <td>40556.000000</td>\n",
       "      <td>40556.000000</td>\n",
       "      <td>40556.000000</td>\n",
       "      <td>40556.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.453817</td>\n",
       "      <td>0.305996</td>\n",
       "      <td>0.558398</td>\n",
       "      <td>2.454898e+05</td>\n",
       "      <td>0.600334</td>\n",
       "      <td>0.181621</td>\n",
       "      <td>0.193823</td>\n",
       "      <td>-9.126433</td>\n",
       "      <td>0.093739</td>\n",
       "      <td>119.875788</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100084</td>\n",
       "      <td>0.099887</td>\n",
       "      <td>0.099837</td>\n",
       "      <td>0.099665</td>\n",
       "      <td>0.099763</td>\n",
       "      <td>0.101070</td>\n",
       "      <td>0.099541</td>\n",
       "      <td>0.100528</td>\n",
       "      <td>0.639757</td>\n",
       "      <td>0.360243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.107851</td>\n",
       "      <td>0.341117</td>\n",
       "      <td>0.178648</td>\n",
       "      <td>1.101264e+05</td>\n",
       "      <td>0.264509</td>\n",
       "      <td>0.325721</td>\n",
       "      <td>0.161508</td>\n",
       "      <td>6.154880</td>\n",
       "      <td>0.101506</td>\n",
       "      <td>30.653724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.300115</td>\n",
       "      <td>0.299852</td>\n",
       "      <td>0.299787</td>\n",
       "      <td>0.299556</td>\n",
       "      <td>0.299688</td>\n",
       "      <td>0.301425</td>\n",
       "      <td>0.299391</td>\n",
       "      <td>0.300706</td>\n",
       "      <td>0.480077</td>\n",
       "      <td>0.480077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059600</td>\n",
       "      <td>1.550900e+04</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009670</td>\n",
       "      <td>-47.046000</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>34.347000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.442000</td>\n",
       "      <td>1.904800e+05</td>\n",
       "      <td>0.433000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097000</td>\n",
       "      <td>-10.843000</td>\n",
       "      <td>0.036100</td>\n",
       "      <td>94.893000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.144000</td>\n",
       "      <td>0.569000</td>\n",
       "      <td>2.275265e+05</td>\n",
       "      <td>0.644000</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.126000</td>\n",
       "      <td>-7.277000</td>\n",
       "      <td>0.048900</td>\n",
       "      <td>119.755500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.687000</td>\n",
       "      <td>2.757600e+05</td>\n",
       "      <td>0.817000</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.244000</td>\n",
       "      <td>-5.174000</td>\n",
       "      <td>0.098800</td>\n",
       "      <td>140.344250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.996000</td>\n",
       "      <td>0.986000</td>\n",
       "      <td>4.497994e+06</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.996000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.744000</td>\n",
       "      <td>0.942000</td>\n",
       "      <td>220.276000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         popularity  acousticness  danceability   duration_ms        energy  \\\n",
       "count  40556.000000  40556.000000  40556.000000  4.055600e+04  40556.000000   \n",
       "mean       1.453817      0.305996      0.558398  2.454898e+05      0.600334   \n",
       "std        1.107851      0.341117      0.178648  1.101264e+05      0.264509   \n",
       "min        0.000000      0.000000      0.059600  1.550900e+04      0.000792   \n",
       "25%        0.000000      0.020000      0.442000  1.904800e+05      0.433000   \n",
       "50%        1.000000      0.144000      0.569000  2.275265e+05      0.644000   \n",
       "75%        2.000000      0.550000      0.687000  2.757600e+05      0.817000   \n",
       "max        3.000000      0.996000      0.986000  4.497994e+06      0.999000   \n",
       "\n",
       "       instrumentalness      liveness      loudness   speechiness  \\\n",
       "count      40556.000000  40556.000000  40556.000000  40556.000000   \n",
       "mean           0.181621      0.193823     -9.126433      0.093739   \n",
       "std            0.325721      0.161508      6.154880      0.101506   \n",
       "min            0.000000      0.009670    -47.046000      0.022300   \n",
       "25%            0.000000      0.097000    -10.843000      0.036100   \n",
       "50%            0.000157      0.126000     -7.277000      0.048900   \n",
       "75%            0.152000      0.244000     -5.174000      0.098800   \n",
       "max            0.996000      1.000000      3.744000      0.942000   \n",
       "\n",
       "              tempo  ...       genre_2       genre_3       genre_4  \\\n",
       "count  40556.000000  ...  40556.000000  40556.000000  40556.000000   \n",
       "mean     119.875788  ...      0.100084      0.099887      0.099837   \n",
       "std       30.653724  ...      0.300115      0.299852      0.299787   \n",
       "min       34.347000  ...      0.000000      0.000000      0.000000   \n",
       "25%       94.893000  ...      0.000000      0.000000      0.000000   \n",
       "50%      119.755500  ...      0.000000      0.000000      0.000000   \n",
       "75%      140.344250  ...      0.000000      0.000000      0.000000   \n",
       "max      220.276000  ...      1.000000      1.000000      1.000000   \n",
       "\n",
       "            genre_5       genre_6       genre_7       genre_8       genre_9  \\\n",
       "count  40556.000000  40556.000000  40556.000000  40556.000000  40556.000000   \n",
       "mean       0.099665      0.099763      0.101070      0.099541      0.100528   \n",
       "std        0.299556      0.299688      0.301425      0.299391      0.300706   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "              Minor         Major  \n",
       "count  40556.000000  40556.000000  \n",
       "mean       0.639757      0.360243  \n",
       "std        0.480077      0.480077  \n",
       "min        0.000000      0.000000  \n",
       "25%        0.000000      0.000000  \n",
       "50%        1.000000      0.000000  \n",
       "75%        1.000000      1.000000  \n",
       "max        1.000000      1.000000  \n",
       "\n",
       "[8 rows x 35 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing the histogram of the data according to their popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Frequency')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(df[\"popularity\"])\n",
    "ax.set_xlabel(\"Popularity\")\n",
    "ax.set_ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have arrived at the threshold from the billboard charts and hence can split the data to two labels. These two labels are popular and non-popular songs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretizing the songs into two labels of popular and non-popular.\n",
    "The songs with \"probability\" attribute less than 42 are non-popular.\n",
    "The other ones are popular\n",
    "The 0th label is the non-popular songs\n",
    "The 1th label is the popular songs \n",
    "So, the dataset is imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    10656\n",
       "2.0    10651\n",
       "1.0    10044\n",
       "3.0     9205\n",
       "Name: popularity, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean_popularity = 45\n",
    "# df[\"popularity\"] = [ 1 if i >= mean_popularity else 0 for i in df.popularity ]\n",
    "df.popularity.value_counts()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping the \"popularity\" attribute from the data frame and declairing it as the dependant variable or the  output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"popularity\"].values\n",
    "X = df.drop([\"popularity\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40556, 34)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into trainig data and testing data. The test size is 0.2 of the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing the dimensions of the arrays produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40556, 34), (40556,))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model with its default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "\n",
    "model = Perceptron()\n",
    "clf = Perceptron(random_state=0).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running randomized gridsearch to reach the best set of hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing gridsearch to find the best hyperparameters for this specific dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haochenyang/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/haochenyang/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/haochenyang/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/haochenyang/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.04005456, 0.03507936, 0.03083348, 0.03724897, 0.03594291,\n",
       "        0.0648669 , 0.03315902, 0.04637408, 0.03395903, 0.02880049,\n",
       "        0.03708601, 0.03608358, 0.06513059, 0.06211984, 0.05791748,\n",
       "        0.05023849, 0.05457163, 0.04477739, 0.05200803, 0.06229866]),\n",
       " 'std_fit_time': array([4.43959236e-03, 1.15358829e-03, 1.01542473e-03, 5.27799129e-03,\n",
       "        1.03622675e-02, 2.57694721e-03, 9.10806656e-03, 5.53131104e-04,\n",
       "        3.86297703e-03, 4.55355644e-03, 7.01808929e-03, 1.00795031e-02,\n",
       "        5.85317612e-05, 1.75191164e-02, 1.53144598e-02, 2.94244289e-03,\n",
       "        7.35449791e-03, 5.13052940e-03, 8.98683071e-03, 2.06714869e-02]),\n",
       " 'mean_score_time': array([0.00641847, 0.00601947, 0.00811303, 0.0056051 , 0.01605606,\n",
       "        0.01195669, 0.02100742, 0.00332606, 0.00178802, 0.00234401,\n",
       "        0.00780916, 0.0028615 , 0.00595844, 0.01929617, 0.00322652,\n",
       "        0.00652206, 0.00497389, 0.01266956, 0.00604236, 0.01213753]),\n",
       " 'std_score_time': array([2.76827812e-03, 2.51352787e-03, 2.84111500e-03, 3.92079353e-04,\n",
       "        1.41682625e-02, 4.84347343e-03, 1.64035559e-02, 1.64902210e-03,\n",
       "        1.91092491e-04, 5.90085983e-05, 6.12592697e-03, 1.38640404e-03,\n",
       "        3.92448902e-03, 7.46703148e-03, 4.73499298e-04, 4.88007069e-03,\n",
       "        2.52008438e-04, 5.60450554e-03, 4.25446033e-03, 3.82745266e-03]),\n",
       " 'param_eta0': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 1,\n",
       "                    1, 1, 1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_iter': masked_array(data=[10, 100, 1000, 10000, 10, 100, 1000, 10000, 10, 100,\n",
       "                    1000, 10000, 10, 100, 1000, 10000, 10, 100, 1000,\n",
       "                    10000],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'eta0': 0.0001, 'max_iter': 10},\n",
       "  {'eta0': 0.0001, 'max_iter': 100},\n",
       "  {'eta0': 0.0001, 'max_iter': 1000},\n",
       "  {'eta0': 0.0001, 'max_iter': 10000},\n",
       "  {'eta0': 0.001, 'max_iter': 10},\n",
       "  {'eta0': 0.001, 'max_iter': 100},\n",
       "  {'eta0': 0.001, 'max_iter': 1000},\n",
       "  {'eta0': 0.001, 'max_iter': 10000},\n",
       "  {'eta0': 0.01, 'max_iter': 10},\n",
       "  {'eta0': 0.01, 'max_iter': 100},\n",
       "  {'eta0': 0.01, 'max_iter': 1000},\n",
       "  {'eta0': 0.01, 'max_iter': 10000},\n",
       "  {'eta0': 0.1, 'max_iter': 10},\n",
       "  {'eta0': 0.1, 'max_iter': 100},\n",
       "  {'eta0': 0.1, 'max_iter': 1000},\n",
       "  {'eta0': 0.1, 'max_iter': 10000},\n",
       "  {'eta0': 1, 'max_iter': 10},\n",
       "  {'eta0': 1, 'max_iter': 100},\n",
       "  {'eta0': 1, 'max_iter': 1000},\n",
       "  {'eta0': 1, 'max_iter': 10000}],\n",
       " 'split0_test_score': array([0.51664406, 0.51664406, 0.51664406, 0.51664406, 0.51664406,\n",
       "        0.51664406, 0.51664406, 0.51664406, 0.51664406, 0.51664406,\n",
       "        0.51664406, 0.51664406, 0.49691777, 0.50092467, 0.50092467,\n",
       "        0.50092467, 0.49691777, 0.4820614 , 0.4820614 , 0.4820614 ]),\n",
       " 'split1_test_score': array([0.49136974, 0.49136974, 0.49136974, 0.49136974, 0.49136974,\n",
       "        0.49136974, 0.49136974, 0.49136974, 0.49136974, 0.49136974,\n",
       "        0.49136974, 0.49136974, 0.48465047, 0.50844532, 0.50844532,\n",
       "        0.50844532, 0.4855135 , 0.49963013, 0.49963013, 0.49963013]),\n",
       " 'mean_test_score': array([0.5040069 , 0.5040069 , 0.5040069 , 0.5040069 , 0.5040069 ,\n",
       "        0.5040069 , 0.5040069 , 0.5040069 , 0.5040069 , 0.5040069 ,\n",
       "        0.5040069 , 0.5040069 , 0.49078412, 0.504685  , 0.504685  ,\n",
       "        0.504685  , 0.49121563, 0.49084577, 0.49084577, 0.49084577]),\n",
       " 'std_test_score': array([0.01263716, 0.01263716, 0.01263716, 0.01263716, 0.01263716,\n",
       "        0.01263716, 0.01263716, 0.01263716, 0.01263716, 0.01263716,\n",
       "        0.01263716, 0.01263716, 0.00613365, 0.00376033, 0.00376033,\n",
       "        0.00376033, 0.00570213, 0.00878437, 0.00878437, 0.00878437]),\n",
       " 'rank_test_score': array([ 4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4, 20,  1,  1,  1, 16,\n",
       "        17, 17, 17], dtype=int32)}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = GridSearchCV(model, {\n",
    "    'eta0' : [ 0.0001, 0.001, 0.01, 0.1, 1], 'max_iter' : [10, 100, 1000, 10000]}, cv = 2, return_train_score = False)\n",
    "\n",
    "clf1.fit(X_train , y_train)\n",
    "clf1.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_eta0</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.040055</td>\n",
       "      <td>0.004440</td>\n",
       "      <td>0.006418</td>\n",
       "      <td>0.002768</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>10</td>\n",
       "      <td>{'eta0': 0.0001, 'max_iter': 10}</td>\n",
       "      <td>0.516644</td>\n",
       "      <td>0.491370</td>\n",
       "      <td>0.504007</td>\n",
       "      <td>0.012637</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.035079</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>0.006019</td>\n",
       "      <td>0.002514</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>100</td>\n",
       "      <td>{'eta0': 0.0001, 'max_iter': 100}</td>\n",
       "      <td>0.516644</td>\n",
       "      <td>0.491370</td>\n",
       "      <td>0.504007</td>\n",
       "      <td>0.012637</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.030833</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.008113</td>\n",
       "      <td>0.002841</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'eta0': 0.0001, 'max_iter': 1000}</td>\n",
       "      <td>0.516644</td>\n",
       "      <td>0.491370</td>\n",
       "      <td>0.504007</td>\n",
       "      <td>0.012637</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.037249</td>\n",
       "      <td>0.005278</td>\n",
       "      <td>0.005605</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>10000</td>\n",
       "      <td>{'eta0': 0.0001, 'max_iter': 10000}</td>\n",
       "      <td>0.516644</td>\n",
       "      <td>0.491370</td>\n",
       "      <td>0.504007</td>\n",
       "      <td>0.012637</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.035943</td>\n",
       "      <td>0.010362</td>\n",
       "      <td>0.016056</td>\n",
       "      <td>0.014168</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10</td>\n",
       "      <td>{'eta0': 0.001, 'max_iter': 10}</td>\n",
       "      <td>0.516644</td>\n",
       "      <td>0.491370</td>\n",
       "      <td>0.504007</td>\n",
       "      <td>0.012637</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.064867</td>\n",
       "      <td>0.002577</td>\n",
       "      <td>0.011957</td>\n",
       "      <td>0.004843</td>\n",
       "      <td>0.001</td>\n",
       "      <td>100</td>\n",
       "      <td>{'eta0': 0.001, 'max_iter': 100}</td>\n",
       "      <td>0.516644</td>\n",
       "      <td>0.491370</td>\n",
       "      <td>0.504007</td>\n",
       "      <td>0.012637</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.033159</td>\n",
       "      <td>0.009108</td>\n",
       "      <td>0.021007</td>\n",
       "      <td>0.016404</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'eta0': 0.001, 'max_iter': 1000}</td>\n",
       "      <td>0.516644</td>\n",
       "      <td>0.491370</td>\n",
       "      <td>0.504007</td>\n",
       "      <td>0.012637</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.046374</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.003326</td>\n",
       "      <td>0.001649</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10000</td>\n",
       "      <td>{'eta0': 0.001, 'max_iter': 10000}</td>\n",
       "      <td>0.516644</td>\n",
       "      <td>0.491370</td>\n",
       "      <td>0.504007</td>\n",
       "      <td>0.012637</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.033959</td>\n",
       "      <td>0.003863</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10</td>\n",
       "      <td>{'eta0': 0.01, 'max_iter': 10}</td>\n",
       "      <td>0.516644</td>\n",
       "      <td>0.491370</td>\n",
       "      <td>0.504007</td>\n",
       "      <td>0.012637</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.028800</td>\n",
       "      <td>0.004554</td>\n",
       "      <td>0.002344</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>{'eta0': 0.01, 'max_iter': 100}</td>\n",
       "      <td>0.516644</td>\n",
       "      <td>0.491370</td>\n",
       "      <td>0.504007</td>\n",
       "      <td>0.012637</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.037086</td>\n",
       "      <td>0.007018</td>\n",
       "      <td>0.007809</td>\n",
       "      <td>0.006126</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'eta0': 0.01, 'max_iter': 1000}</td>\n",
       "      <td>0.516644</td>\n",
       "      <td>0.491370</td>\n",
       "      <td>0.504007</td>\n",
       "      <td>0.012637</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.036084</td>\n",
       "      <td>0.010080</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10000</td>\n",
       "      <td>{'eta0': 0.01, 'max_iter': 10000}</td>\n",
       "      <td>0.516644</td>\n",
       "      <td>0.491370</td>\n",
       "      <td>0.504007</td>\n",
       "      <td>0.012637</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.065131</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.005958</td>\n",
       "      <td>0.003924</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>{'eta0': 0.1, 'max_iter': 10}</td>\n",
       "      <td>0.496918</td>\n",
       "      <td>0.484650</td>\n",
       "      <td>0.490784</td>\n",
       "      <td>0.006134</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.062120</td>\n",
       "      <td>0.017519</td>\n",
       "      <td>0.019296</td>\n",
       "      <td>0.007467</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>{'eta0': 0.1, 'max_iter': 100}</td>\n",
       "      <td>0.500925</td>\n",
       "      <td>0.508445</td>\n",
       "      <td>0.504685</td>\n",
       "      <td>0.003760</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.057917</td>\n",
       "      <td>0.015314</td>\n",
       "      <td>0.003227</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'eta0': 0.1, 'max_iter': 1000}</td>\n",
       "      <td>0.500925</td>\n",
       "      <td>0.508445</td>\n",
       "      <td>0.504685</td>\n",
       "      <td>0.003760</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.050238</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.006522</td>\n",
       "      <td>0.004880</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10000</td>\n",
       "      <td>{'eta0': 0.1, 'max_iter': 10000}</td>\n",
       "      <td>0.500925</td>\n",
       "      <td>0.508445</td>\n",
       "      <td>0.504685</td>\n",
       "      <td>0.003760</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.054572</td>\n",
       "      <td>0.007354</td>\n",
       "      <td>0.004974</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>{'eta0': 1, 'max_iter': 10}</td>\n",
       "      <td>0.496918</td>\n",
       "      <td>0.485514</td>\n",
       "      <td>0.491216</td>\n",
       "      <td>0.005702</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.044777</td>\n",
       "      <td>0.005131</td>\n",
       "      <td>0.012670</td>\n",
       "      <td>0.005605</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>{'eta0': 1, 'max_iter': 100}</td>\n",
       "      <td>0.482061</td>\n",
       "      <td>0.499630</td>\n",
       "      <td>0.490846</td>\n",
       "      <td>0.008784</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.052008</td>\n",
       "      <td>0.008987</td>\n",
       "      <td>0.006042</td>\n",
       "      <td>0.004254</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'eta0': 1, 'max_iter': 1000}</td>\n",
       "      <td>0.482061</td>\n",
       "      <td>0.499630</td>\n",
       "      <td>0.490846</td>\n",
       "      <td>0.008784</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.062299</td>\n",
       "      <td>0.020671</td>\n",
       "      <td>0.012138</td>\n",
       "      <td>0.003827</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>{'eta0': 1, 'max_iter': 10000}</td>\n",
       "      <td>0.482061</td>\n",
       "      <td>0.499630</td>\n",
       "      <td>0.490846</td>\n",
       "      <td>0.008784</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_eta0  \\\n",
       "0        0.040055      0.004440         0.006418        0.002768     0.0001   \n",
       "1        0.035079      0.001154         0.006019        0.002514     0.0001   \n",
       "2        0.030833      0.001015         0.008113        0.002841     0.0001   \n",
       "3        0.037249      0.005278         0.005605        0.000392     0.0001   \n",
       "4        0.035943      0.010362         0.016056        0.014168      0.001   \n",
       "5        0.064867      0.002577         0.011957        0.004843      0.001   \n",
       "6        0.033159      0.009108         0.021007        0.016404      0.001   \n",
       "7        0.046374      0.000553         0.003326        0.001649      0.001   \n",
       "8        0.033959      0.003863         0.001788        0.000191       0.01   \n",
       "9        0.028800      0.004554         0.002344        0.000059       0.01   \n",
       "10       0.037086      0.007018         0.007809        0.006126       0.01   \n",
       "11       0.036084      0.010080         0.002861        0.001386       0.01   \n",
       "12       0.065131      0.000059         0.005958        0.003924        0.1   \n",
       "13       0.062120      0.017519         0.019296        0.007467        0.1   \n",
       "14       0.057917      0.015314         0.003227        0.000473        0.1   \n",
       "15       0.050238      0.002942         0.006522        0.004880        0.1   \n",
       "16       0.054572      0.007354         0.004974        0.000252          1   \n",
       "17       0.044777      0.005131         0.012670        0.005605          1   \n",
       "18       0.052008      0.008987         0.006042        0.004254          1   \n",
       "19       0.062299      0.020671         0.012138        0.003827          1   \n",
       "\n",
       "   param_max_iter                               params  split0_test_score  \\\n",
       "0              10     {'eta0': 0.0001, 'max_iter': 10}           0.516644   \n",
       "1             100    {'eta0': 0.0001, 'max_iter': 100}           0.516644   \n",
       "2            1000   {'eta0': 0.0001, 'max_iter': 1000}           0.516644   \n",
       "3           10000  {'eta0': 0.0001, 'max_iter': 10000}           0.516644   \n",
       "4              10      {'eta0': 0.001, 'max_iter': 10}           0.516644   \n",
       "5             100     {'eta0': 0.001, 'max_iter': 100}           0.516644   \n",
       "6            1000    {'eta0': 0.001, 'max_iter': 1000}           0.516644   \n",
       "7           10000   {'eta0': 0.001, 'max_iter': 10000}           0.516644   \n",
       "8              10       {'eta0': 0.01, 'max_iter': 10}           0.516644   \n",
       "9             100      {'eta0': 0.01, 'max_iter': 100}           0.516644   \n",
       "10           1000     {'eta0': 0.01, 'max_iter': 1000}           0.516644   \n",
       "11          10000    {'eta0': 0.01, 'max_iter': 10000}           0.516644   \n",
       "12             10        {'eta0': 0.1, 'max_iter': 10}           0.496918   \n",
       "13            100       {'eta0': 0.1, 'max_iter': 100}           0.500925   \n",
       "14           1000      {'eta0': 0.1, 'max_iter': 1000}           0.500925   \n",
       "15          10000     {'eta0': 0.1, 'max_iter': 10000}           0.500925   \n",
       "16             10          {'eta0': 1, 'max_iter': 10}           0.496918   \n",
       "17            100         {'eta0': 1, 'max_iter': 100}           0.482061   \n",
       "18           1000        {'eta0': 1, 'max_iter': 1000}           0.482061   \n",
       "19          10000       {'eta0': 1, 'max_iter': 10000}           0.482061   \n",
       "\n",
       "    split1_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.491370         0.504007        0.012637                4  \n",
       "1            0.491370         0.504007        0.012637                4  \n",
       "2            0.491370         0.504007        0.012637                4  \n",
       "3            0.491370         0.504007        0.012637                4  \n",
       "4            0.491370         0.504007        0.012637                4  \n",
       "5            0.491370         0.504007        0.012637                4  \n",
       "6            0.491370         0.504007        0.012637                4  \n",
       "7            0.491370         0.504007        0.012637                4  \n",
       "8            0.491370         0.504007        0.012637                4  \n",
       "9            0.491370         0.504007        0.012637                4  \n",
       "10           0.491370         0.504007        0.012637                4  \n",
       "11           0.491370         0.504007        0.012637                4  \n",
       "12           0.484650         0.490784        0.006134               20  \n",
       "13           0.508445         0.504685        0.003760                1  \n",
       "14           0.508445         0.504685        0.003760                1  \n",
       "15           0.508445         0.504685        0.003760                1  \n",
       "16           0.485514         0.491216        0.005702               16  \n",
       "17           0.499630         0.490846        0.008784               17  \n",
       "18           0.499630         0.490846        0.008784               17  \n",
       "19           0.499630         0.490846        0.008784               17  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Printing the accuracy scores for different permutations of the parameters eta0 and maximum iterations\n",
    "\n",
    "df = pd.DataFrame(clf1.cv_results_)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the classifier with the best set of hyperparameter (eta0 = 1 and maximum iterations = 10000)\n",
    "Similar results were found with different permutations of these two hyperparameters. But we kept the maximum iterations (number of learning epochs) at its highest possible value for the sake of convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model with the best set of hyperparameters\n",
    "eta0 = 1, maximum iterations = 10000\n",
    "Different permutations of hyperparameters gave the same mean_test_score\n",
    "We kept the maximum iterations at its highest possible value for the sake of convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Perceptron(random_state=0, max_iter=10000, eta0 = 0.1).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4616570090001233"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the y value with the aforementioned hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4614151873767258"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing confusion matrix to see the accuracy of the actual positives, the false positives and actual negatives and the false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1167  780   92  119]\n",
      " [ 511 1011  359  153]\n",
      " [ 155  525  737  637]\n",
      " [  51  146  841  828]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing the classification report to see the precision and recall for the different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.54      0.58      2158\n",
      "         1.0       0.41      0.50      0.45      2034\n",
      "         2.0       0.36      0.36      0.36      2054\n",
      "         3.0       0.48      0.44      0.46      1866\n",
      "\n",
      "    accuracy                           0.46      8112\n",
      "   macro avg       0.47      0.46      0.46      8112\n",
      "weighted avg       0.47      0.46      0.46      8112\n",
      "\n",
      "Classification report\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Classification report\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
